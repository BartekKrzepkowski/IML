{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab7 - MC Dropout\n",
        "\n",
        "In today's lab we will implement Monte-Carlo droput for neural network and use it as a model specific informativeness measure in an active learning cycle.\n",
        "\n",
        "As shown by [Gal & Ghahramani (2016)]( https://arxiv.org/abs/1506.02142)\n",
        "Dropout in Neural Network can be used as an approximation of Bayesian model and therefore can be used as a measure of models uncertainty.\n",
        "\n",
        "Lets start by loading Fashion Mnist dataset and creating a simple NN with pytorch."
      ],
      "metadata": {
        "id": "KXG1KB3kY8aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "          nn.Conv2d(1,32, 3),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2),\n",
        "          nn.Dropout(),\n",
        "          nn.Conv2d(32,64, 3),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2),\n",
        "          nn.Dropout(),\n",
        "          nn.Conv2d(64, 32, 3),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2),\n",
        "          nn.Flatten(),\n",
        "          nn.Dropout(),\n",
        "          nn.Linear(32, 10)\n",
        "        )\n",
        "model = model.float()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "k1f5myfWgeuy"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0,), (1,))\n",
        "    ])\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0,), (1,))\n",
        "    ])\n",
        ")\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "jFxumKpslxjz"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can define below functions for training and evaluation with created NN."
      ],
      "metadata": {
        "id": "1T8Kn-ykE14U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, num_epochs=1):\n",
        "    size = len(dataloader.dataset)\n",
        "    for epoch in range(num_epochs):\n",
        "      for batch, (X, y) in enumerate(dataloader):\n",
        "          pred = model(X)\n",
        "          loss = loss_fn(pred, y)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if batch % 100 == 0:\n",
        "              loss, current = loss.item(), batch * len(X)\n",
        "              print(f\"Epoch {epoch}, loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, labels_estimated, correct_labels = 0, [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct_labels.extend(y.numpy())\n",
        "            labels_estimated.extend((pred.argmax(1)).numpy())\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    print(f\"Test Error: \\n BAC: {balanced_accuracy_score(correct_labels, labels_estimated)} \\n Loss {test_loss}\")"
      ],
      "metadata": {
        "id": "rDPoIBnpo-QQ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loop(train_dataloader, model, loss_fn, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJo5zsFUp1VS",
        "outputId": "5156e74c-2445-4f77-f0a7-768c544c43ba"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss: 2.309655  [    0/60000]\n",
            "Epoch 0, loss: 1.558943  [ 6400/60000]\n",
            "Epoch 0, loss: 1.217630  [12800/60000]\n",
            "Epoch 0, loss: 1.297080  [19200/60000]\n",
            "Epoch 0, loss: 1.228658  [25600/60000]\n",
            "Epoch 0, loss: 0.991111  [32000/60000]\n",
            "Epoch 0, loss: 0.864235  [38400/60000]\n",
            "Epoch 0, loss: 1.047429  [44800/60000]\n",
            "Epoch 0, loss: 0.783615  [51200/60000]\n",
            "Epoch 0, loss: 0.858984  [57600/60000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loop(test_dataloader, model, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhhwEshcpkCb",
        "outputId": "4bd257e1-1aaf-43e2-f0e1-ae5ff2d52430"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " BAC: 0.6368 \n",
            " Loss 0.9636115507715067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create a function that measures the standard deviation of predictions using proposed model on given samples with dropout enabled."
      ],
      "metadata": {
        "id": "DE6M3kW2FapZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VNQgefLLFyx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement BALD informativeness:\n",
        "$$\n",
        "u^*_{BALD} = \\arg\\max_{u} H(y|u, U_{tr}) - E_{\\theta\\sim p(\\theta|U_{tr})}[H(y|u,\\theta) ]\n",
        "$$\n",
        "where $H$ is entropy function, $U_{tr}$ is current training set and  $Î¸$ are the parameters of the model. \n",
        "\n",
        "\n",
        "To obtain the first part of total uncertainty run normal inference with the model, estimate the second part by making inference with dropout in \"training\" mode.\n",
        "\n",
        "Warning: Make sure that values that you are applying entropy function to proper probability distributions. "
      ],
      "metadata": {
        "id": "8Z1vv91BFz5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uI8-pOu16xyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Prepare an active learning experiment, split the training dataset into 1.5% of randomly selected initial training data and and a pool from which our algorithms will chose samples."
      ],
      "metadata": {
        "id": "hfhTCSnl6yEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qeyiCQ8iF3O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Use created informativeness functions in active learning experiment. Choose 5 batches with 64 samples in each batch. Compare obtained results with random sampling."
      ],
      "metadata": {
        "id": "F_sVy7JPJExN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mFr8If9oJm1G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}