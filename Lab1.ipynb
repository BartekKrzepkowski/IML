{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXW8FDx8byk6"
   },
   "source": [
    "# Lab1 - Learning with limited data\n",
    "\n",
    "In this scenario we will investigate the performance of various models when limited training labelling capabilities are available. \n",
    "\n",
    "Lets start from loading a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRfWcbgceRG-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MeVVkSTbxUh"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "dataset = datasets.fetch_covtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNt35G7Wd4-k",
    "outputId": "cc601bfb-5e26-4467-f13d-0a19dc4a69f2"
   },
   "outputs": [],
   "source": [
    "X, y = dataset.data, dataset.target\n",
    "print(f'Descriptive features shape: {X.shape}, target variable shape: {y.shape}')\n",
    "y_unique, y_counts = np.unique(y, return_counts=True)\n",
    "print(f'Unique targets:')\n",
    "print(y_unique)\n",
    "print(f'Distribution of targets: {y_counts/y.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAorEGkpi3La"
   },
   "source": [
    "### Dataset description\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/Covertype\n",
    "\n",
    "\\\" Given is the attribute name, attribute type, the measurement unit and a brief description. The forest cover type is the classification problem. The order of this listing corresponds to the order of numerals along the rows of the database.\n",
    "\n",
    "Name / Data Type / Measurement / Description\n",
    "\n",
    "Elevation / quantitative /meters / Elevation in meters \\\\\n",
    "Aspect / quantitative / azimuth / Aspect in degrees azimuth \\\\\n",
    "Slope / quantitative / degrees / Slope in degrees \\\\\n",
    "Horizontal_Distance_To_Hydrology / quantitative / meters / Horz Dist to nearest surface water features \\\\\n",
    "Vertical_Distance_To_Hydrology / quantitative / meters / Vert Dist to nearest surface water features \\\\\n",
    "Horizontal_Distance_To_Roadways / quantitative / meters / Horz Dist to nearest roadway \\\\\n",
    "Hillshade_9am / quantitative / 0 to 255 index / Hillshade index at 9am, summer solstice \\\\\n",
    "Hillshade_Noon / quantitative / 0 to 255 index / Hillshade index at noon, summer soltice \\\\\n",
    "Hillshade_3pm / quantitative / 0 to 255 index / Hillshade index at 3pm, summer solstice \\\\\n",
    "Horizontal_Distance_To_Fire_Points / quantitative / meters / Horz Dist to nearest wildfire ignition points \\\\\n",
    "Wilderness_Area (4 binary columns) / qualitative / 0 (absence) or 1 (presence) / Wilderness area designation \\\\\n",
    "Soil_Type (40 binary columns) / qualitative / 0 (absence) or 1 (presence) / Soil Type designation \\\\\n",
    "\n",
    "Target variable:\n",
    "\n",
    "Cover_Type (7 types) / integer / 1 to 7 / Forest Cover Type designation \\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "3e3ldt7FqFHo",
    "outputId": "a838e49e-a7e1-4e79-facf-489c932d12e0"
   },
   "outputs": [],
   "source": [
    "! pip install umap-learn\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_subset = X[:(len(X)//10)]\n",
    "\n",
    "plot_data = umap.UMAP(n_neighbors=10).fit_transform(StandardScaler().fit_transform(data_subset))\n",
    "plt.scatter(plot_data[:, 0], plot_data[:, 1], c=y[:(len(X)//10)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1d8jdeGkRmK"
   },
   "source": [
    "\n",
    "---\n",
    "1. Split the dataset into training and evaluatuion part, keep the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51x1RCoSeWth"
   },
   "outputs": [],
   "source": [
    "# use 40% of the data for evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aedeLoL9krOL"
   },
   "source": [
    "\n",
    "2. Train a simple model - logistic regression on the whole training part of the dataset. Measure its performance using BAC. \n",
    "\n",
    "Should the data be preprocessed in any way?\n",
    "\n",
    "This is the performance that we are trying to reach when limited labelling capabilites are available. It can be considered as an upper bound, although in some cases, e.g. model overfitting, better performance can be obtained with less labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIPmKnoWlMOU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pWQOOdCal8J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPafS3_ZlNA3"
   },
   "source": [
    "Now we will try to simulate the fact tha we have limited labelling capabilites by \"forgetting\" part of the labels. In practice in the active learning scenario we can query experts only for some of the labels, the rest of the targets are not available. \\\\\n",
    "k = 0.1 \\\\\n",
    "3. Sample k% of the training data without returning. Train the same model on the chosen samples, measure the performance. Repeat this procedure 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Plisy4q1mhWs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9eyUPTwmy_k"
   },
   "source": [
    "4. Repeat the same procedure from point 3 for k = 0.1, 0.2, ..., 0.9. Plot the model performance with respect to the part of data used for training. Add a horizontal line for model performance on the whole training dataset for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eh-UQ1sqnXRD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrsDjtgfndad"
   },
   "source": [
    "5. Repeat the steps from point 3 and 4 but now sample in the stratified manner. Compare the model results with proper plot. Can this be done in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JH3hLITzncip"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8q2py7Ei1lo"
   },
   "source": [
    "6*. Design a better method of choosing the training sample. Plot it with random methods for comparisson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3l0vwF13nnLI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pack_exc",
   "language": "python",
   "name": "pack_exc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
